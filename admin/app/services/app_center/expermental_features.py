import json
from datetime import datetime
from app.app import app, db
from app.models.next_console.next_console_model import NextConsoleMessage
from app.services.next_console.llm import NextConsoleLLMClient
from app.services.next_console.next_console import next_console_response
from concurrent.futures import wait


def parallel_llm_node_execute(params, task_record, global_params):
    """
    å¹³è¡Œæ‰§è¡Œ LLM èŠ‚ç‚¹
        [] æ ¹æ®é˜Ÿåˆ—å±æ€§ï¼Œè¿›è¡Œä¸€æ¬¡å¾ªç¯ï¼Œç»„è£…llm-parmamå˜é‡
        [] å…¨éƒ¨æ‰§è¡Œå®Œæˆåæ±‡æ€»
        [] å…¶ä½™æ€§è´¨ç›¸åŒï¼Œä¸æ”¯æŒ
    """
    from app.services.app_center.llm_node_service import load_llm_prams
    workflow_node_llm_params = load_llm_prams(params, task_record, global_params)
    llm_client = NextConsoleLLMClient(workflow_node_llm_params)
    if not llm_client.llm_client:
        app.logger.error(f"å·¥ä½œæµå‘ç°æ­¤æ¨¡å‹{task_record.workflow_node_llm_code}æš‚ä¸æ”¯æŒ")
        task_record.task_status = "å¼‚å¸¸"
        task_record.task_trace_log = f"å·¥ä½œæµå‘ç°æ­¤æ¨¡å‹{task_record.workflow_node_llm_code}æš‚ä¸æ”¯æŒ"
        db.session.add(task_record)
        db.session.commit()
        return next_console_response(error_status=True,
                                     error_message=f"æ­¤æ¨¡å‹{task_record.workflow_node_llm_code}æš‚ä¸æ”¯æŒ")
    parallel_attr = task_record.workflow_node_run_model_config.get("parallel_attr", "")
    if not parallel_attr or not isinstance(params[parallel_attr], list):
        raise ValueError("parallel_attr must be a list of attributes to parallelize.")
    msg_content = ""
    reasoning_content = ""
    msg_token_used = 0
    sub_results = []
    for item in params[parallel_attr]:
        new_sub_params = {k: params[k] for k in params if k not in params[parallel_attr]}
        new_sub_params[parallel_attr] = [item]
        workflow_node_llm_params = load_llm_prams(new_sub_params, task_record, global_params)
        future = global_params["executor"].submit(single_llm_sub_node_execute,
                                                  llm_client, workflow_node_llm_params,
                                                  task_record.to_dict(), global_params)
        # æ·»åŠ å›è°ƒç¡®ä¿å­ä»»åŠ¡å®Œæˆï¼ˆå¯é€‰ï¼‰
        future.add_done_callback(
            lambda f: print(f"ğŸ¯agent_run_workflow:result: {f.result()}") if f.exception() is None
            else print(f"agent_run_workflow:âŒerror: {f.exception()}"))
        sub_results.append(future)
    # ç­‰å¾…æ‰€æœ‰å­ä»»åŠ¡å®Œæˆ
    wait(sub_results)
    for future in sub_results:
        try:
            msg_content_part, reasoning_content_part, msg_token_used_part, answer_msg = future.result()
            msg_content += msg_content_part
            reasoning_content += reasoning_content_part
            msg_token_used += msg_token_used_part
        except Exception as e:
            app.logger.error(f"æ‰§è¡Œ LLM å­èŠ‚ç‚¹æ—¶å‘ç”Ÿå¼‚å¸¸ï¼š{str(e)}")
            msg_content += "\n\n **å¯¹ä¸èµ·ï¼Œæ¨¡å‹æœåŠ¡æ­£å¿™ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»åé‡è¯•ï¼Œæˆ–è€…å¯ä»¥è¯•è¯•åˆ‡æ¢å…¶ä»–æ¨¡å‹~**"
    task_record.task_result = json.dumps({
        "content": msg_content,
        "reasoning_content": reasoning_content,
    })
    task_record.end_time = datetime.now()
    task_record.task_status = "å·²å®Œæˆ"
    task_record.task_token_used = msg_token_used
    db.session.add(task_record)
    db.session.commit()
    return True


def single_llm_sub_node_execute(llm_client, workflow_node_llm_params, task_record, global_params):
    """
    æ‰§è¡Œå•ä¸ª LLM å­èŠ‚ç‚¹
    """
    with app.app_context():
        msg_content = ""
        reasoning_content = ""
        msg_token_used = 0
        answer_msg = None
        if workflow_node_llm_params.get("stream", False):
            all_message_format = [msg_schema.get("schema_type")
                                  for msg_schema in task_record.get("workflow_node_message_schema")]
            output_flag = task_record.get("workflow_node_enable_message") and global_params[
                "stream"] and 'messageFlow' in all_message_format
            if output_flag:
                answer_msg = NextConsoleMessage(
                    user_id=task_record.get("user_id"),
                    session_id=task_record.get("session_id"),
                    qa_id=task_record.get("qa_id"),
                    msg_format='messageFlow',
                    msg_llm_type=task_record.get("workflow_node_llm_code"),
                    msg_role="assistant",
                    msg_parent_id=task_record.get("msg_id"),
                    msg_content=''
                )
                db.session.add(answer_msg)
                db.session.commit()
            # æµå¼æ‰§è¡Œ
            try:
                completion = llm_client.chat(workflow_node_llm_params)
                for chunk in completion:
                    if global_params.get("stop_flag"):
                        raise GeneratorExit
                    if hasattr(chunk, "usage") and chunk.usage and chunk.usage.total_tokens > 0:
                        msg_token_used = chunk.usage.total_tokens
                    if chunk.choices and (
                            chunk.choices[0].delta.content or hasattr(chunk.choices[0].delta, "reasoning_content")):
                        if hasattr(chunk.choices[0].delta, "reasoning_content") and chunk.choices[0].delta.reasoning_content:
                            reasoning_content += chunk.choices[0].delta.reasoning_content
                        elif chunk.choices[0].delta.content:
                            msg_content += chunk.choices[0].delta.content
                        if global_params["stream"] and output_flag:
                            chunk_res = chunk.model_dump_json()
                            chunk_res = json.loads(chunk_res)
                            chunk_res["session_id"] = task_record.get("session_id"),
                            chunk_res["qa_id"] = task_record.get("qa_id"),
                            chunk_res["msg_parent_id"] = task_record.get("msg_id"),
                            chunk_res["msg_id"] = answer_msg.msg_id
                            global_params["message_queue"].put(chunk_res)
            except GeneratorExit:
                pass
            except Exception as e3:
                app.logger.error(f"è°ƒç”¨åŸºæ¨¡å‹å¼‚å¸¸ï¼š{str(e3)}")
                msg_content += "\n\n **å¯¹ä¸èµ·ï¼Œæ¨¡å‹æœåŠ¡æ­£å¿™ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»åé‡è¯•ï¼Œæˆ–è€…å¯ä»¥è¯•è¯•åˆ‡æ¢å…¶ä»–æ¨¡å‹~**"
                if task_record.get("workflow_node_enable_message") and global_params["stream"]:
                    if task_record.get("workflow_node_message_schema_type") == "messageFlow":
                        except_result = {
                            "id": "",
                            "session_id": task_record.get("session_id"),
                            "qa_id": task_record.get("qa_id"),
                            "msg_parent_id": task_record.get("msg_id"),
                            "created": 0,
                            "model": '',
                            "object": "chat.completion",
                            "choices": [
                                {
                                    "finish_reason": "error",
                                    "index": 0,
                                    "delta": {
                                        "content": msg_content,
                                        "role": "assistant"
                                    },

                                }
                            ]
                        }
                        global_params["message_queue"].put(except_result)
            finally:
                if output_flag:
                    answer_msg.msg_content = msg_content
                    answer_msg.reasoning_content = reasoning_content
                    answer_msg.msg_token_used = msg_token_used
                    db.session.add(answer_msg)
                    db.session.commit()
            return msg_content, reasoning_content, msg_token_used, answer_msg
        else:
            # éæµå¼æ‰§è¡Œ
            try:
                res = llm_client.chat(workflow_node_llm_params).model_dump_json()
                res = json.loads(res)
                msg_content = res.get("choices")[0].get("message").get("content")
                reasoning_content = res.get("choices")[0].get("reasoning_content", "")
                msg_token_used = res.get("usage", {}).get("total_tokens", 0)
            except Exception as e:
                msg_content = 'å¯¹ä¸èµ·ï¼Œæ¨¡å‹æœåŠ¡æ­£å¿™ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»åé‡è¯•ï¼Œæˆ–è€…å¯ä»¥è¯•è¯•åˆ‡æ¢å…¶ä»–æ¨¡å‹~'
            return msg_content, reasoning_content, msg_token_used, answer_msg

